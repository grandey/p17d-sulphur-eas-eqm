#+TITLE: p17d-sulphur-eas-eqm
#+AUTHOR: Benjamin S. Grandey
#+OPTIONS: ^:nil

** Purpose
Investigate the *equilibrium* climate response to *sulphur* emissions from power plants and industry in *East and Southeast Asia*, using CESM1.2.2(CAM5).

** Experimental design

*** Emissions scenarios
Two emissions scenarios will be investigated:
1. =year-2000=, which follows the default MAM3 emissions for the =F_2000_CAM5= (=FC5=) compset.
2. =eas0=, which is similar to the above, but with no sulphur (SO2 and SO4) emissions from the energy and industry sectors over East and Southeast Asia (94E-161E, 10S-65N). The following emissions files will be modified: =ar5_mam3_so2_elev_2000_c090726.nc=, =ar5_mam3_so4_a1_elev_2000_c090726.nc=, and =ar5_mam3_num_a1_elev_2000_c090726.nc=.

*** Prescribed-SST (F) simulations
In order to quantify radiative effects, two prescribed-SST simulations will be performed:
1. =p17d_f_2000=, which uses the =year-2000= emissions.
2. =p17d_f_eas0=, which uses the =eas0= emissions.

These prescribed-SST simulations will be configured as follows:
1. Greenhouse gas concentrations and sea-surface temperatures (SSTs) will be prescribed using *year-2000 climatological values*, based on the =F_2000_CAM5= (=FC5=) [[http://www.cesm.ucar.edu/models/cesm1.2/cesm/doc/modelnl/compsets.html][compset]].
2. The *RFP components* will be diagnosed following [[http://www.atmos-chem-phys.net/13/9971/2013/][Ghan (2013)]]. The "online" radiation call will include aerosol-radiation interactions; the "offline" diagnostic radiation call will calculate "clean-sky" fluxes.
3. A resolution of *f19_g16* will be specified.
4. Each simulation will be run for *32 years*, and the first two years will be excluded as spin-up. (Nudging will not be performed.)
5. The simulations will be performed on *Cheyenne*, using 720 processors (20 nodes). Model throughput is approximately 47 years/day, with a cost of 360-370 pe-hrs/yr.

Details of the configuration can be found in [[https://github.com/grandey/p17d-sulphur-eas-eqm/blob/master/config_simulations/config_f_simulations.org][config_f_simulations.org]].

*** Equilibrium coupled atmosphere-ocean (B) simulations
In order to investigate the equilibrium climate response, two coupled atmosphere-ocean simulations will be performed:
1. =p17d_b_2000=, which uses the =year-2000= emissions.
2. =p17d_b_eas0=, which uses the =eas0= emissions.

These atmosphere-ocean simulations will be configured as follows:
1. Greenhouse gas concentrations will be prescribed using *year-2000 climatological values*, based on the =B_2000_CAM5_CN= (=BC5CN=) [[http://www.cesm.ucar.edu/models/cesm1.2/cesm/doc/modelnl/compsets.html][compset]].
2. Data for *dynamical downscaling* will be produced.
3. A resolution of *f19_g16* will be specified.
4. Each simulation will be run for *100 years*, allowing several decades to be excluded as spin-up.
5. The simulations will be performed on *Cheyenne*, using 128 processors for the ocean in addition to 720 processors for the other model components (24 nodes total). Model throughput is approximately 30 years/day, with a cost of 630-660 pe-hrs/yr.

Details of the configuration can be found in [[https://github.com/grandey/p17d-sulphur-eas-eqm/blob/master/config_simulations/config_b_simulations.org][config_b_simulations.org]].

** Data management

*** Size of atmosphere-ocean (B) output
Due to the large size of the atmospheric downscaling data and the ocean data, each coupled atmosphere-ocean simulation produces approximately 300 GB/decade (3 TB/century). Of this, approximately 120 GB is atmospheric downscaling data, 130 GB is monthly ocean data, and 50 GB is other data.

By gzipping the data, the size of the atmospheric downscaling data files can be reduced by 15% and the monthly ocean data files by almost 60%. Hence, gzipping should reduce the overall data size to approximately 200 GB/decade (2 TB/century).

In order to further reduce the storage requirements, the downscaling and ocean data from the first four decades can be deleted (see below).

*** Saving a copy of the output data to Newton
On Cheyenne, a copy of the case directory for each simulation will be rsynced to the archive directory, e.g.:

#+BEGIN_SRC
CASENAME=p17d_f_2000
ARCHIVE_DIR=/glade/scratch/bgrandey/archive
rsync -av /glade/u/home/bgrandey/cesm1_2_2_1_cases/$CASENAME $ARCHIVE_DIR/$CASENAME/arch_case/
#+END_SRC

On Cheyenne, the NetCDF files in the archive directory will be gzipped:

#+BEGIN_SRC
for f in $ARCHIVE_DIR/$CASENAME/*/*/*.nc; do if [ -f $f ]; then echo "Gzipping $f"; gzip -f $f; fi; done
#+END_SRC

Atmospheric downscaling and monthly ocean data from the first four decades (actually 39 years) can be deleted:

#+BEGIN_SRC
rm -f $ARCHIVE_DIR/$CASENAME/atm/hist/$CASENAME.cam.h1.00[0123]?-*.nc.gz
rm -f $ARCHIVE_DIR/$CASENAME/ocn/hist/$CASENAME.pop.h.00[0123]?-*.nc.gz
#+END_SRC

A copy of the data will then be rsynced from Cheyenne to Newton:

#+BEGIN_SRC
rsync -avz --progress -e "ssh -p $NEWTON_PORT" $ARCHIVE_DIR/$CASENAME $NEWTON_USER@$NEWTON_IP:/somerset/grandey/data4/acrc/RUN/archive/
#+END_SRC

Transfer speeds are approximately 0.5 MB/s (40 GB/day). The transfer can be sped up by running several rsync tasks concurrently, with each task transferring a subset of the data, e.g.:

#+BEGIN_SRC
rsync -avz --progress -e "ssh -p $NEWTON_PORT" $ARCHIVE_DIR/$CASENAME/atm/hist/$CASENAME.cam.h0.*.nc.gz $NEWTON_USER@$NEWTON_IP:/somerset/grandey/data4/acrc/RUN/archive/$CASENAME/atm/hist/

rsync -avz --progress -e "ssh -p $NEWTON_PORT" $ARCHIVE_DIR/$CASENAME/atm/hist/$CASENAME.cam.h1.0???-0[123]-*.nc.gz $NEWTON_USER@$NEWTON_IP:/somerset/grandey/data4/acrc/RUN/archive/$CASENAME/atm/hist/

rsync -avz --progress -e "ssh -p $NEWTON_PORT" $ARCHIVE_DIR/$CASENAME/ocn/hist/$CASENAME.pop.h.nday1.*.nc.gz $NEWTON_USER@$NEWTON_IP:/somerset/grandey/data4/acrc/RUN/archive/$CASENAME/ocn/hist/

rsync -avz --progress -e "ssh -p $NEWTON_PORT" $ARCHIVE_DIR/$CASENAME/ocn/hist/$CASENAME.pop.h.0???-0[123].nc.gz $NEWTON_USER@$NEWTON_IP:/somerset/grandey/data4/acrc/RUN/archive/$CASENAME/ocn/hist/

for M in lnd ice; do echo $M; rsync -avz --progress -e "ssh -p $NEWTON_PORT" $ARCHIVE_DIR/$CASENAME/$M/hist/$CASENAME.*.nc.gz $NEWTON_USER@$NEWTON_IP:/somerset/grandey/data4/acrc/RUN/archive/$CASENAME/$M/hist/; done
#+END_SRC

*** Converting from time-slice to time-series format
After gunzipping a copy of the atmospheric h0 files on Newton, [[https://github.com/NCAR/PyReshaper][PyReshaper]] (v1.0.1) can be used to convert to time-series format. I have =PyReshaper= installed in a separate =conda= environment:

#+BEGIN_SRC
source activate pyreshaper
#+END_SRC

First, =s2make= is used to generate a specifier file, e.g.:

#+BEGIN_SRC
CASENAME=p17d_f_2000

IN_DIR=/somerset/grandey/data4/acrc/RUN/unzipped/$CASENAME/atm/hist
OUT_DIR=/dhobyghaut/grandey/data5/cesm/s2s/$CASENAME/atm

mkdir -p $OUT_DIR

s2smake \
    --netcdf_format="netcdf4" \
    --compression_level=1 \
    --output_prefix="$OUT_DIR/$CASENAME.cam.h0." \
    --output_suffix=".nc" \
    -m "time" -m "time_bnds" -m "ch4vmr" -m "co2vmr" -m "f11vmr" \
    -m "time_written" -m "n2ovmr" -m "date_written" -m "f12vmr" \
    -m "sol_tsi" -m "nsteph" -m "datesec" -m "ndcur" -m "date" \
    -m "nscur" \
    --specfile=$OUT_DIR/specfile_$CASENAME.s2s \
    $IN_DIR/$CASENAME.cam.h0.????-??.nc
#+END_SRC

(The metadata field information (indicated by =m=) has been copied from some example code Daniel Rothenberg kindly provided.)

Second, =s2run= is run in parallel in order to convert the data to time-series format:

#+BEGIN_SRC
mpirun -n 8 s2srun --verbosity=2 $OUT_DIR/specfile_$CASENAME.s2s
#+END_SRC

** Syncing to local machine for analysis
Data of interest can then be pulled from Newton using rsync.

Prescribed-SST radiation data:

#+BEGIN_SRC
CASENAME_LIST="p17d_f_2000 p17d_f_eas0"
VARIABLE_LIST="FSNTOA FSNTOA_d1 FSNTOAC_d1 SWCF_d1 LWCF LWCF_d1"
#+END_SRC

Rsync command:

#+BEGIN_SRC
for CASENAME in $CASENAME_LIST
do
  for VARIABLE in $VARIABLE_LIST
  do
    rsync -av --progress -e "ssh -p $NEWTON_PORT" \
        $NEWTON_USER@$NEWTON_IP:/dhobyghaut/grandey/data5/cesm/s2s/$CASENAME/atm/$CASENAME.cam.h0.$VARIABLE.nc \
        $HOME/data/projects/p17d_sulphur_eas_eqm/output_timeseries/
  done
done
#+END_SRC

** Status

*** Completed
***** DONE Prepare modified *emissions* for =eas0= scenario.
CLOSED: [2017-08-14 Mon 16:03]
***** DONE Design and check =user_nl_cam= files for *prescribed-SST* simulations
CLOSED: [2017-08-14 Mon 16:12]
***** DONE Design and check configuration of *prescribed-SST* simulations
CLOSED: [2017-08-14 Mon 16:17]
***** DONE Create and submit *prescribed-SST* simulations
CLOSED: [2017-08-14 Mon 16:42]
***** DONE Design and check =user_nl_cam= files for coupled *atmosphere-ocean* simulations
CLOSED: [2017-08-15 Tue 14:22]
***** DONE Design and check configuration of coupled *atmosphere-ocean* simulations
CLOSED: [2017-08-15 Tue 14:25]
***** DONE Create and submit coupled *atmosphere-ocean* simulations
CLOSED: [2017-08-15 Tue 14:44]
***** DONE Archive copy of *prescribed-SST output* to Newton
CLOSED: [2017-08-16 Wed 10:40]
***** DONE Archive copy of coupled *atmosphere-ocean output* to Newton
CLOSED: [2017-08-21 Mon 14:06]
***** DONE Check that no downscaling data files are missing on Newton 
CLOSED: [2017-08-21 Mon 11:54]
***** DONE AMWG diagnostics for prescribed-SST simulations
CLOSED: [2017-08-21 Mon 15:55]
***** DONE AMWG diagnostics for coupled atmosphere-ocean simulations
CLOSED: [2017-08-21 Mon 15:55]

*** Still to-do
***** TODO Convert atm h0 data from simulations to time-series format on Newton

** Author
Benjamin S. Grandey, 2017, in collaboration with Yeo Lik Khian, Lee Hsiang-He, and [[http://web.mit.edu/wangc/][Chien Wang]].

** Acknowledgements
This repository has been developed in order to facilitate research conducted at the Singapore-MIT Alliance for Research and Technology (SMART), supported by the National Research Foundation (NRF), Prime Ministerâ€™s Office, Singapore under its Campus for Research Excellence and Technological Enterprise (CREATE) programme.

